{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiran-kmr1810/SRGAN/blob/main/SRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5_F_M6hyVXJ"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations==0.4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg19\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "S6woCMbfylTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = True\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_GEN = \"/content/gdrive/MyDrive/gen.pth\"\n",
        "CHECKPOINT_DISC = \"/content/gdrive/MyDrive/disc.pth\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 1000\n",
        "BATCH_SIZE = 16\n",
        "LAMBDA_GP = 10\n",
        "NUM_WORKERS = 4\n",
        "HIGH_RES = 64\n",
        "LOW_RES = HIGH_RES // 4\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "highres_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "lowres_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=LOW_RES, height=LOW_RES, interpolation=Image.BICUBIC),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "both_transforms = A.Compose(\n",
        "    [\n",
        "        A.RandomCrop(width=HIGH_RES, height=HIGH_RES),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "yggBAnsOyl8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "HRDPVBr4ysrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x /content/gdrive/MyDrive/Ground_Truth.rar '/content/gdrive/MyDrive/BSDS100_dataset'"
      ],
      "metadata": {
        "id": "IwiukWsc6CrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_act, **kwargs):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            **kwargs,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.act = nn.LeakyReLU(0.2, inplace=True) if use_act else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.cnn(x))\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_c, scale_factor=2):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
        "        self.conv = nn.Conv2d(in_c, in_c, 3, 1, 1, bias=True)\n",
        "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.conv(self.upsample(x)))\n",
        "\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, channels=32, residual_beta=0.2):\n",
        "        super().__init__()\n",
        "        self.residual_beta = residual_beta\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        for i in range(5):\n",
        "            self.blocks.append(\n",
        "                ConvBlock(\n",
        "                    in_channels + channels * i,\n",
        "                    channels if i <= 3 else in_channels,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                    use_act=True if i <= 3 else False,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(new_inputs)\n",
        "            new_inputs = torch.cat([new_inputs, out], dim=1)\n",
        "        return self.residual_beta * out + x\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, in_channels, residual_beta=0.2):\n",
        "        super().__init__()\n",
        "        self.residual_beta = residual_beta\n",
        "        self.rrdb = nn.Sequential(*[DenseResidualBlock(in_channels) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.rrdb(x) * self.residual_beta + x\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_channels=64, num_blocks=23):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Conv2d(\n",
        "            in_channels,\n",
        "            num_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.residuals = nn.Sequential(*[RRDB(num_channels) for _ in range(num_blocks)])\n",
        "        self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.upsamples = nn.Sequential(\n",
        "            UpsampleBlock(num_channels), UpsampleBlock(num_channels),\n",
        "        )\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, num_channels, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(num_channels, in_channels, 3, 1, 1, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        initial = self.initial(x)\n",
        "        x = self.conv(self.residuals(initial)) + initial\n",
        "        x = self.upsamples(x)\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        for idx, feature in enumerate(features):\n",
        "            blocks.append(\n",
        "                ConvBlock(\n",
        "                    in_channels,\n",
        "                    feature,\n",
        "                    kernel_size=3,\n",
        "                    stride=1 + idx % 2,\n",
        "                    padding=1,\n",
        "                    use_act=True,\n",
        "                ),\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((6, 6)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 6 * 6, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def initialize_weights(model, scale=0.1):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight.data)\n",
        "            m.weight.data *= scale\n",
        "\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight.data)\n",
        "            m.weight.data *= scale\n",
        "\n",
        "\n",
        "def test():\n",
        "    gen = Generator()\n",
        "    disc = Discriminator()\n",
        "    low_res = 24\n",
        "    x = torch.randn((5, 3, low_res, low_res))\n",
        "    gen_out = gen(x)\n",
        "    disc_out = disc(gen_out)\n",
        "\n",
        "    print(gen_out.shape)\n",
        "    print(disc_out.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "id": "59RVjFx_y2QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.augmentations.transforms import ImageCompression\n",
        "from PIL import Image\n",
        "\n",
        "def gradient_penalty(critic, real, fake, device):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"gen.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location = DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def plot_examples(low_res_folder, gen):\n",
        "    files = os.listdir(low_res_folder)\n",
        "\n",
        "    gen.eval()\n",
        "    for file in files:\n",
        "        image = Image.open(r\"/content/gdrive/MyDrive/inputs/\" + file)\n",
        "        image.show()\n",
        "        with torch.no_grad():\n",
        "            upscaled_img = gen(\n",
        "                test_transform(image=np.asarray(image))[\"image\"]\n",
        "                .unsqueeze(0)\n",
        "                .to(DEVICE)\n",
        "            )\n",
        "        save_image(upscaled_img, f\"/content/gdrive/MyDrive/saved/{file}\")\n",
        "    gen.train()"
      ],
      "metadata": {
        "id": "Euen3lvDfFrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import ThroughputBenchmark\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def train_fn(\n",
        "    loader,\n",
        "    disc,\n",
        "    gen,\n",
        "    opt_gen,\n",
        "    opt_disc,\n",
        "    l1,\n",
        "    vgg_loss,\n",
        "    g_scaler,\n",
        "    d_scaler,\n",
        "    writer,\n",
        "    tb_step,\n",
        "):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (low_res, high_res) in enumerate(loop):\n",
        "        high_res = high_res.to(DEVICE)\n",
        "        low_res = low_res.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake = gen(low_res)\n",
        "            critic_real = disc(high_res)\n",
        "            critic_fake = disc(fake.detach())\n",
        "            gp = gradient_penalty(disc, high_res, fake, device=DEVICE)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "                + LAMBDA_GP * gp\n",
        "            )\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(loss_critic).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            l1_loss = 1e-2 * l1(fake, high_res)\n",
        "            adversarial_loss = 5e-3 * -torch.mean(disc(fake))\n",
        "            loss_for_vgg = vgg_loss(fake, high_res)\n",
        "            gen_loss = l1_loss + loss_for_vgg + adversarial_loss\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(gen_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        writer.add_scalar(\"Critic loss\", loss_critic.item(), global_step=tb_step)\n",
        "        tb_step += 1\n",
        "\n",
        "        if idx % 100 == 0 and idx > 0:\n",
        "            plot_examples(\"/content/gdrive/MyDrive/inputs/\", gen)\n",
        "\n",
        "        loop.set_postfix(\n",
        "            gp=gp.item(),\n",
        "            critic=loss_critic.item(),\n",
        "            l1=l1_loss.item(),\n",
        "            vgg=loss_for_vgg.item(),\n",
        "            adversarial=adversarial_loss.item(),\n",
        "        )\n",
        "\n",
        "    return tb_step\n",
        "\n",
        "\n",
        "def main():\n",
        "    dataset = MyImageFolder(root_dir=\"/content/gdrive/MyDrive/BSDS100_dataset/\")\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers = NUM_WORKERS,\n",
        "    )\n",
        "    gen = Generator(in_channels=3).to(DEVICE)\n",
        "    disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "    initialize_weights(gen)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr = LEARNING_RATE, betas=(0.0, 0.9))\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr = LEARNING_RATE, betas=(0.0, 0.9))\n",
        "    writer = SummaryWriter(\"logs\")\n",
        "    tb_step = 0\n",
        "    l1 = nn.L1Loss()\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "    vgg_loss = VGGLoss()\n",
        "\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_GEN,\n",
        "            gen,\n",
        "            opt_gen,\n",
        "            LEARNING_RATE,\n",
        "        )\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_DISC,\n",
        "            disc,\n",
        "            opt_disc,\n",
        "            LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        tb_step = train_fn(\n",
        "            loader,\n",
        "            disc,\n",
        "            gen,\n",
        "            opt_gen,\n",
        "            opt_disc,\n",
        "            l1,\n",
        "            vgg_loss,\n",
        "            g_scaler,\n",
        "            d_scaler,\n",
        "            writer,\n",
        "            tb_step,\n",
        "        )\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            save_checkpoint(gen, opt_gen, filename = CHECKPOINT_GEN)\n",
        "            save_checkpoint(disc, opt_disc, filename = CHECKPOINT_DISC)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run = True\n",
        "\n",
        "    if run:\n",
        "        gen = Generator(in_channels=3).to(DEVICE)\n",
        "        opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_GEN,\n",
        "            gen,\n",
        "            opt_gen,\n",
        "            LEARNING_RATE,\n",
        "        )\n",
        "        plot_examples(\"/content/gdrive/MyDrive/inputs/\", gen)\n",
        "    \n",
        "    else:\n",
        "        print(\"train\")\n",
        "        main()"
      ],
      "metadata": {
        "id": "DZABR7NczPNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyImageFolder(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        super(MyImageFolder, self).__init__()\n",
        "        self.data = []\n",
        "        self.root_dir = root_dir\n",
        "        self.class_names = os.listdir(root_dir)\n",
        "\n",
        "        for index, name in enumerate(self.class_names):\n",
        "            files = os.listdir(os.path.join(root_dir, name))\n",
        "            self.data += list(zip(files, [index] * len(files)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_file, label = self.data[index]\n",
        "        root_and_dir = os.path.join(self.root_dir, self.class_names[label])\n",
        "\n",
        "        image = cv2.imread(os.path.join(root_and_dir, img_file))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        both_transform = both_transforms(image=image)[\"image\"]\n",
        "        low_res = lowres_transform(image=both_transform)[\"image\"]\n",
        "        high_res = highres_transform(image=both_transform)[\"image\"]\n",
        "        return low_res, high_res\n",
        "\n",
        "\n",
        "def test():\n",
        "    dataset = MyImageFolder(root_dir=\"/content/gdrive/MyDrive/BSDS100_dataset/\")\n",
        "    loader = DataLoader(dataset,batch_size = 100)\n",
        "\n",
        "    for low_res, high_res in loader:\n",
        "        print(low_res.shape)\n",
        "        print(high_res.shape)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "id": "2KPO-Vnqyu2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vgg = vgg19(pretrained=True).features[:35].eval().to(DEVICE)\n",
        "\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        vgg_input_features = self.vgg(input)\n",
        "        vgg_target_features = self.vgg(target)\n",
        "        return self.loss(vgg_input_features, vgg_target_features)"
      ],
      "metadata": {
        "id": "KfuVjxDgy1W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True : pass"
      ],
      "metadata": {
        "id": "yRljczFEkPyu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}